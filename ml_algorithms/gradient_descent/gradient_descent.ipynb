{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf93499-fa61-4dcc-92f6-53b30c6d87ee",
   "metadata": {},
   "source": [
    "### **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d0017-d1cc-4384-a642-164ceeeaea8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989a1c8-8d30-4938-bbda-ab9df53e7c3a",
   "metadata": {},
   "source": [
    "The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a434dc-4d8b-40c9-8148-5aa7668df9d1",
   "metadata": {},
   "source": [
    "#### **Intuition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dafeb66-e03d-4cfa-b670-6e56ff09e3f5",
   "metadata": {},
   "source": [
    "The intuition is to find the slope of the equation that needs to be minimised, at a specific point and moving in the direction opposite to this spole in order to find a point where the slope is close to or equal to **zero**. \n",
    "\n",
    "If the slope is positive, then the equation is increasing at that point, and we move in the backwards direction.\n",
    "\n",
    "But if the slope is negative, then the equation is decreasing at that point, and we need to move in the forward direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057317af-bbe5-42a0-9f09-e55117640f47",
   "metadata": {},
   "source": [
    "Let the equation be dependent on just one variable, say $b$, then -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399c941-805d-463d-95ed-1064525144cb",
   "metadata": {},
   "source": [
    "### $$ loss= f(b) $$\n",
    "\n",
    "### $$ b_{new} = b_{old} - f^\\prime(b_{old}) $$\n",
    "\n",
    "### $$ b_{new} = b_{old} - \\eta * f^\\prime(b_{old}) $$\n",
    "\n",
    "where, \n",
    "\n",
    "### $$\\eta = learning\\_rate(hyper\\_parameter) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb0824-54e3-416a-92f3-5f362f163c7e",
   "metadata": {},
   "source": [
    "Convex functions have only one minima (global minima)\n",
    "\n",
    "Non-convex points have many minima, and can lead to the gradient descent converging at a local minima instead of the global minima.\n",
    "\n",
    "Properly scaled data leads to faster convergance.\n",
    "\n",
    "<img src=\"../../assets/convex_concave.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c5405-da68-4a94-95c3-eed4a8958f56",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e438e33",
   "metadata": {},
   "source": [
    "### **Gradient descent for linear regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204228a1",
   "metadata": {},
   "source": [
    "### $$ y = \\sum_{i=0}^n \\beta_i X_i $$\n",
    "\n",
    "where,\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "        1 & x_{11} & ... & x_{1m}\\\\\n",
    "        1 & x_{21} & ... & x_{2m}\\\\\n",
    "        \\vdots\\\\\n",
    "        1 & x_{n1} & ... & x_{nm}\n",
    "      \\end{bmatrix} $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2c7f7",
   "metadata": {},
   "source": [
    "### **$$ {{\\delta L}\\over{\\delta \\beta m}} = {{-2}\\over{n}}\\sum (y_i - \\hat{y_i})X_{im}$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35e536-5252-4b43-bd39-3371bb6a3f72",
   "metadata": {},
   "source": [
    "### Types of Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c9d42-3a09-4f19-86d0-6fa98138dae0",
   "metadata": {},
   "source": [
    "1. Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9efd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs### **$$ Loss = (XW- Y) ^ T (XW - Y) + \\lambda W^T W$$**\n",
    "\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            # update all the coef and the intercept\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "            \n",
    "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "            \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d7df85",
   "metadata": {},
   "source": [
    "2. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094db399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            for j in range(X_train.shape[0]):\n",
    "                idx = np.random.randint(0,X_train.shape[0])\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                \n",
    "                intercept_der = -2 * (y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "                \n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "            \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf27be1",
   "metadata": {},
   "source": [
    "3. Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361cfd2-6def-4d72-9a0a-b2b69f51ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MBGDRegressor:\n",
    "    \n",
    "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                \n",
    "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                #print(\"Shape of y_hat\",y_hat.shape)\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "            \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30d6a6-7233-4b27-b648-f891bf1aa75b",
   "metadata": {},
   "source": [
    "<img src=\"../../assets/diff_gd.png\"/>\n",
    "<img src=\"../../assets/diff_gd_cost.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c81ce9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
